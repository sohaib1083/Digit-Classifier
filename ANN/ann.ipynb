{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "y08TQBDjCBW_"
   },
   "source": [
    "#### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "unEIOJ3KBtX8"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wIGCb5qBG9K2"
   },
   "source": [
    "#### Defining activation functions and their derivatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "T_YFxI9WHFA8"
   },
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(x * -1))\n",
    "\n",
    "def tanh(x):\n",
    "    return (np.exp(x) - np.exp(x * -1)) / (np.exp(x) + np.exp(x * -1))\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(x, 0)\n",
    "\n",
    "def dSigmoid(x):\n",
    "    y = sigmoid(x)\n",
    "    return y * (1 - y)\n",
    "\n",
    "def dTanh(x):\n",
    "    y = tanh(x)\n",
    "    return 1 - y ** 2\n",
    "\n",
    "def dRelu(x):\n",
    "    return np.array(x >= 0, dtype='float64')\n",
    "\n",
    "activation_function_dictionary = {\n",
    "    'sigmoid': {\n",
    "        'function': sigmoid,\n",
    "        'derivative': dSigmoid\n",
    "    },\n",
    "    'relu': {\n",
    "        'function': relu,\n",
    "        'derivative': dRelu\n",
    "    },\n",
    "    'tanh': {\n",
    "        'function': tanh,\n",
    "        'derivative': dTanh\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "j6HjkVZaQsQK"
   },
   "source": [
    "#### Defining initializers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YXFFxQtKQyob"
   },
   "outputs": [],
   "source": [
    "def zero_initialization(dim1, dim2):\n",
    "    return np.zeros((dim1, dim2))\n",
    "\n",
    "def random_initializtion(dim1, dim2):\n",
    "    np.random.seed(0)\n",
    "    return np.random.randn(dim1, dim2)\n",
    "\n",
    "def xavier_initialization(units, prev_units):\n",
    "    np.random.seed(0)\n",
    "    return np.random.randn(units, prev_units) * ((1 / prev_units) ** 0.5)\n",
    "\n",
    "def he_initialization(units, prev_units):\n",
    "    np.random.seed(0)\n",
    "    return np.random.randn(units, prev_units) * ((2 / prev_units) ** 0.5)\n",
    "\n",
    "initialization_dictionary = {\n",
    "    'zero': zero_initialization,\n",
    "    'random': random_initializtion,\n",
    "    'xavier': xavier_initialization,\n",
    "    'he': he_initialization\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "f8jjopdlZN2A"
   },
   "source": [
    "#### Defining Cost Functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-nIDlYe1ZUrA"
   },
   "outputs": [],
   "source": [
    "def mse(outputs, labels):\n",
    "     m = np.size(outputs, axis=1)\n",
    "     cost = np.sum((outputs - labels) ** 2) / (2 * m)\n",
    "     return cost\n",
    "\n",
    "def mse_derivative(outputs, labels):\n",
    "    m = np.size(outputs, axis=1)\n",
    "    outputs = outputs + 10 ** -12\n",
    "    return (outputs - labels) / m\n",
    "\n",
    "def cross_entropy(outputs, labels):\n",
    "    m = np.size(outputs, axis=1)\n",
    "    cost = -np.sum((labels * np.log(outputs + 10 ** -12)) + ((1 - labels) * \n",
    "                   np.log((1 - outputs) + 10 ** -12))) \n",
    "    return cost / m\n",
    "\n",
    "def cross_entropy_derivative(outputs, labels):\n",
    "    m = np.size(outputs, axis=1)\n",
    "    outputs = outputs + 10 ** -12\n",
    "    return ((1 - labels) / (1 - outputs) - labels / outputs) / m\n",
    "\n",
    "def regularization(regularization_constant, layers):\n",
    "    m = np.size(layers[0].parameters['W'], axis=1)\n",
    "    regularization_cost = 0\n",
    "    for layer in layers:\n",
    "        regularization_cost += (np.sum(layer.parameters['W'] ** 2) + \n",
    "                                    np.sum(layer.parameters['b'] ** 2)) \n",
    "    return regularization_cost * (regularization_constant / (2 * m))\n",
    "\n",
    "def regularization_derivative(regularization_constant, layers):\n",
    "    m = np.size(layers[0].parameters['W'], axis=1)\n",
    "    return [\n",
    "            {\n",
    "                'W': layer.parameters['W'] * (regularization_constant / m), \n",
    "                'b': layer.parameters['b'] * (regularization_constant / m)\n",
    "            }\n",
    "            for layer in layers\n",
    "    ]\n",
    "    \n",
    "cost_functions_dictionary = {\n",
    "    'mse': {\n",
    "        'function': mse,\n",
    "        'derivative': mse_derivative\n",
    "    },\n",
    "    'cross_entropy': {\n",
    "        'function': cross_entropy,\n",
    "        'derivative': cross_entropy_derivative\n",
    "    },\n",
    "    'regularization': {\n",
    "        'function': regularization,\n",
    "        'derivative': regularization_derivative\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "is4tq6qcGm6S"
   },
   "source": [
    "#### Creating Layer classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Bd1tiDqBGraq"
   },
   "outputs": [],
   "source": [
    "class Layer:\n",
    "\n",
    "    def __init__(self, layer_type):\n",
    "        self.layer_type = layer_type\n",
    "    \n",
    "    def forward_propagate(self, X, predict=False):\n",
    "        pass\n",
    "    \n",
    "    def backward_propagate(self, dA, X):\n",
    "        pass\n",
    "\n",
    "\n",
    "class Dense(Layer):\n",
    "\n",
    "    def __init__(self, activation, units, prev_units, initializer='xavier'):\n",
    "        super().__init__('Dense')\n",
    "        assert activation in activation_function_dictionary\n",
    "        assert initializer in initialization_dictionary\n",
    "        self.function = activation_function_dictionary[activation]['function']\n",
    "        self.derivative = activation_function_dictionary[activation]['derivative']\n",
    "        self.parameters = {\n",
    "            'W': initialization_dictionary[initializer](units, prev_units),\n",
    "            'b': initialization_dictionary['zero'](units, 1)\n",
    "        }\n",
    "\n",
    "    def forward_propagate(self, X, predict=False):\n",
    "        self.pre_activations = np.dot(self.parameters['W'], X) + self.parameters['b']\n",
    "        self.activations = self.function(self.pre_activations)\n",
    "        return self.activations\n",
    "    \n",
    "    def backward_propagate(self, dA, X):\n",
    "        dZ = dA * self.derivative(self.pre_activations)\n",
    "        dW = np.dot(dZ, X.T)\n",
    "        db = np.sum(dZ, axis=1, keepdims=True)\n",
    "        dA = np.dot(self.parameters['W'].T, dZ)\n",
    "        return dW, db, dA\n",
    "\n",
    "\n",
    "class BatchNorm(Layer):\n",
    "    \n",
    "    def __init__(self, prev_units):\n",
    "        super().__init__('BatchNorm')\n",
    "        self.units = prev_units\n",
    "        self.parameters = {\n",
    "            'W': np.ones((self.units, 1)),\n",
    "            'b': np.zeros((self.units, 1))\n",
    "        }\n",
    "        self.batch_count = 0\n",
    "\n",
    "    def forward_propagate(self, X, predict=False):\n",
    "        if(self.batch_count == 0):\n",
    "            self.avg_mean = np.zeros((X.shape[0], 1))\n",
    "            self.avg_dev = np.zeros((X.shape[0], 1))\n",
    "        if(not predict):\n",
    "            self.batch_mean = np.mean(X, axis=1, keepdims=True)\n",
    "            self.batch_dev = (np.var(X, axis=1, keepdims=True) + 10 ** -12) ** 0.5\n",
    "            self.avg_mean += self.batch_mean\n",
    "            self.avg_dev += self.batch_dev\n",
    "            self.batch_count += 1\n",
    "            self.activations = (self.parameters['W'] * ((X - self.batch_mean) / \n",
    "                                        self.batch_dev) + self.parameters['b'])\n",
    "        else:\n",
    "            self.activations = (self.parameters['W'] * ((X - (self.avg_mean / \n",
    "                                    self.batch_count)) / (self.avg_dev / \n",
    "                                    self.batch_count)) + self.parameters['b'])\n",
    "        return self.activations\n",
    "\n",
    "    def backward_propagate(self, dA, X):\n",
    "        m = np.size(X, axis=1)\n",
    "        dW = np.sum(dA * ((X - self.batch_mean) / self.batch_dev), axis=1, keepdims=True)\n",
    "        db = np.sum(dA, axis=1, keepdims=True)\n",
    "        dA = dA * (self.parameters['W'] * (((m - 1) / m) * (1 - (1 / m) * (((X - \n",
    "                        self.batch_mean) / self.batch_dev) ** 2)) *  (1 / self.batch_dev)))\n",
    "        return dW, db, dA\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kUTYfs4HC1sr"
   },
   "source": [
    "#### Creating ANN class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_Rb-PB-ACel9"
   },
   "outputs": [],
   "source": [
    "class ANN:\n",
    "\n",
    "    @staticmethod\n",
    "    def shuffle(X, Y):\n",
    "        X_shuffled = X.copy()\n",
    "        Y_shuffled = Y.copy()\n",
    "        for i in range(np.size(X_shuffled, axis=1)):\n",
    "            np.random.seed(0)\n",
    "            swap_index = np.random.randint(i, np.size(X_shuffled, axis=1))\n",
    "            X_temp = X_shuffled[:, i]\n",
    "            Y_temp = Y_shuffled[:, i]\n",
    "            X_shuffled[:, i] = X_shuffled[:, swap_index]\n",
    "            Y_shuffled[:, i] = Y_shuffled[:, swap_index]\n",
    "            X_shuffled[:, swap_index] = X_temp\n",
    "            Y_shuffled[:, swap_index] = Y_temp\n",
    "        return X_shuffled, Y_shuffled\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def get_mini_batch_splits(mini_batch_size, dataset_size):\n",
    "        splits = []\n",
    "        index = 0\n",
    "        while index < dataset_size:\n",
    "            splits.append((index, min(index + mini_batch_size - 1, dataset_size - 1)))\n",
    "            index += mini_batch_size\n",
    "        return splits\n",
    "\n",
    "\n",
    "    def __init__(self, input_size, cost_function):\n",
    "        assert cost_function in cost_functions_dictionary\n",
    "        self.cost_function = cost_functions_dictionary[cost_function]\n",
    "        self.last_input_size = input_size\n",
    "        self.layers = []\n",
    "\n",
    "\n",
    "    def add_layer_Dense(self, activation, units, initializer='xavier'):\n",
    "        self.layers.append(Dense(activation, units, self.last_input_size, initializer))\n",
    "        self.last_input_size = units\n",
    "    \n",
    "\n",
    "    def add_layer_BatchNorm(self):\n",
    "        self.layers.append(BatchNorm(self.last_input_size))\n",
    "\n",
    "\n",
    "    def descend_with_batch(self, X_batch, Y_batch, learning_rate, beta1, beta2, \n",
    "        regularization_constant, V_W, V_b, S_W, S_b):\n",
    "        outputs = X_batch\n",
    "        for j in range(len(self.layers)):\n",
    "            outputs = self.layers[j].forward_propagate(outputs)\n",
    "\n",
    "        regularization_derivatives = (cost_functions_dictionary['regularization']['derivative']\n",
    "                                      (regularization_constant, self.layers))\n",
    "        dA = self.cost_function['derivative'](outputs, Y_batch)\n",
    "        for j in range(-1, -len(self.layers) - 1, -1):\n",
    "            dW, db, dA = self.layers[j].backward_propagate(dA, X_batch if \n",
    "                j == -len(self.layers) else self.layers[j - 1].activations)\n",
    "            dW = dW + regularization_derivatives[j]['W']\n",
    "            db = db + regularization_derivatives[j]['b']\n",
    "            \n",
    "            V_W[j]['updates'] += 1\n",
    "            V_W[j]['values'] = beta1 * V_W[j]['values'] + dW\n",
    "            V_W[j]['values'] = V_W[j]['values'] * ((1 - beta1) / (1 - beta1 ** (V_W[j]['updates'])))\n",
    "            V_b[j]['updates'] += 1\n",
    "            V_b[j]['values'] = beta1 * V_b[j]['values'] + db\n",
    "            V_b[j]['values'] = V_b[j]['values'] * ((1 - beta1) / (1 - beta1 ** (V_b[j]['updates'])))\n",
    "            S_W[j]['updates'] += 1\n",
    "            S_W[j]['values'] = beta2 * S_W[j]['values'] + dW ** 2\n",
    "            S_W[j]['values'] = S_W[j]['values'] * ((1 - beta2) / (1 - beta2 ** (S_W[j]['updates'])))\n",
    "            S_b[j]['updates'] += 1\n",
    "            S_b[j]['values'] = beta2 * S_b[j]['values'] + db ** 2\n",
    "            S_b[j]['values'] = S_b[j]['values'] * ((1 - beta2) / (1 - beta2 ** (S_b[j]['updates'])))\n",
    "\n",
    "            self.layers[j].parameters['W'] = (self.layers[j].parameters['W'] - learning_rate * \n",
    "                            (V_W[j]['values'] / (S_W[j]['values'] ** 0.5 + 10 ** -12)))\n",
    "            self.layers[j].parameters['b'] = (self.layers[j].parameters['b'] - learning_rate * \n",
    "                            (V_b[j]['values'] / (S_b[j]['values'] ** 0.5 + 10 ** -12)))\n",
    "\n",
    "            self.layers[j].parameters['W'] += 10 ** -12\n",
    "            self.layers[j].parameters['b'] += 10 ** -12\n",
    "\n",
    "\n",
    "    def fit(self, X, Y, num_iterations=100, learning_rate=0.001, beta1=0.9, \n",
    "        beta2=0.999, regularization_constant=0, mini_batch_size = 512):\n",
    "        assert self.layers[0].parameters['W'].shape[1] == X.shape[0]\n",
    "        X, Y = self.shuffle(X ,Y)\n",
    "        X = self.normalize(X)\n",
    "\n",
    "        V_W = [{'values': np.zeros(layer.parameters['W'].shape), 'updates': 0} \n",
    "               for layer in self.layers]\n",
    "        V_b = [{'values': np.zeros(layer.parameters['b'].shape), 'updates': 0} \n",
    "               for layer in self.layers]\n",
    "        S_W = [{'values': np.zeros(layer.parameters['W'].shape), 'updates': 0} \n",
    "               for layer in self.layers]\n",
    "        S_b = [{'values': np.zeros(layer.parameters['b'].shape), 'updates': 0} \n",
    "               for layer in self.layers]\n",
    "        \n",
    "        if mini_batch_size == -1:\n",
    "            mini_batch_size = np.size(X, axis=1)\n",
    "        splits = self.get_mini_batch_splits(mini_batch_size, np.size(X, axis=1))\n",
    "        costs = []\n",
    "        for i in range(num_iterations + 1):\n",
    "            for split in splits:\n",
    "                X_batch = X[:, split[0]:split[1] + 1]\n",
    "                Y_batch = Y[:, split[0]:split[1] + 1]\n",
    "                self.descend_with_batch(X_batch, Y_batch, learning_rate, beta1, \n",
    "                            beta2, regularization_constant, V_W, V_b, S_W, S_b)\n",
    "\n",
    "            if (i + 1) % 5 == 0 or i + 1 == 1:\n",
    "                outputs = X\n",
    "                for j in range(len(self.layers)):\n",
    "                    outputs = self.layers[j].forward_propagate(outputs)\n",
    "                cost = round(self.cost_function['function'](outputs, Y) + \n",
    "                            cost_functions_dictionary['regularization']['function']\n",
    "                            (regularization_constant, self.layers), 4)\n",
    "                print('Cost after epoch #{} = {}'.format(i + 1, cost))\n",
    "                costs.append(cost)\n",
    "\n",
    "        plt.plot([i + 1 for i in range(len(costs))], costs, color='black')\n",
    "        plt.title('Cost Variation')\n",
    "        plt.ylabel('Costs')\n",
    "        plt.show()\n",
    "    \n",
    "\n",
    "    def normalize(self, X):\n",
    "        self.means = np.mean(X, axis=1)\n",
    "        self.scales = np.amax(X, axis=1) - np.amin(X, axis=1)\n",
    "        for i in range(len(self.scales)):\n",
    "            if self.scales[i] == 0:\n",
    "                self.scales[i] = 1\n",
    "        self.means = self.means.reshape((np.size(X, axis=0), 1))\n",
    "        self.scales = self.scales.reshape((np.size(X, axis=0), 1))\n",
    "        X = (X - self.means) / self.scales\n",
    "        return X\n",
    "    \n",
    "\n",
    "    def predict(self, X):\n",
    "        X = (X - self.means) / self.scales\n",
    "        outputs = X\n",
    "        for i in range(len(self.layers)):\n",
    "            outputs = self.layers[i].forward_propagate(outputs, predict=True)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Utility functions for interacting with the dataset \"\"\"\n",
    "\n",
    "def get_training_image_vector(image_number):\n",
    "    file = open('../Dataset/train-images-idx3-ubyte', 'rb')\n",
    "    file.seek(16 + (image_number - 1) * 784, 0)\n",
    "\n",
    "    image_vector = []\n",
    "    for i in range(0, 784):\n",
    "        byte = int.from_bytes(file.read(1), byteorder='big')\n",
    "        image_vector.append(byte)\n",
    "\n",
    "    file.close()\n",
    "    return image_vector\n",
    "\n",
    "\n",
    "def get_training_image_label(image_number):\n",
    "    file = open('../Dataset/train-labels-idx1-ubyte', 'rb')\n",
    "    file.seek(8 + (image_number - 1), 0)\n",
    "\n",
    "    byte = int.from_bytes(file.read(1), byteorder='big')\n",
    "    image_label = byte\n",
    "\n",
    "    file.close()\n",
    "    return image_label\n",
    "\n",
    "def get_test_image_vector(image_number):\n",
    "    file = open('../Dataset/t10k-images-idx3-ubyte', 'rb')\n",
    "    file.seek(16 + (image_number - 1) * 784, 0)\n",
    "\n",
    "    image_vector = []\n",
    "    for i in range(0, 784):\n",
    "        byte = int.from_bytes(file.read(1), byteorder='big')\n",
    "        image_vector.append(byte)\n",
    "\n",
    "    file.close()\n",
    "    return image_vector\n",
    "\n",
    "\n",
    "def get_test_image_label(image_number):\n",
    "    file = open('../Dataset/t10k-labels-idx1-ubyte', 'rb')\n",
    "    file.seek(8 + (image_number - 1), 0)\n",
    "\n",
    "    byte = int.from_bytes(file.read(1), byteorder='big')\n",
    "    image_label = byte\n",
    "\n",
    "    file.close()\n",
    "    return image_label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GrG7vZcdCPu7"
   },
   "source": [
    "#### Getting Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "N4O3K375CO1C"
   },
   "outputs": [],
   "source": [
    "TRAINING_EXAMPLES = 60000\n",
    "TEST_EXAMPLES = 10000\n",
    "INPUT_FEATURES = 784\n",
    "\n",
    "X_train = np.ones((TRAINING_EXAMPLES, INPUT_FEATURES))\n",
    "y_train = np.ones(TRAINING_EXAMPLES)\n",
    "\n",
    "for i in range(TRAINING_EXAMPLES):\n",
    "    X_train[i, 0:] = get_training_image_vector(i + 1)\n",
    "    y_train[i] = get_training_image_label(i + 1)\n",
    "\n",
    "y_vectors = np.zeros((len(y_train), 10))\n",
    "for i in range(len(y_train)):\n",
    "    y_vectors[i, int(y_train[i])] = 1\n",
    "    \n",
    "X_train = X_train.T\n",
    "Y_train = y_vectors.T\n",
    "\n",
    "X_test = np.ones((TEST_EXAMPLES, INPUT_FEATURES))\n",
    "y_test = np.ones(TEST_EXAMPLES)\n",
    "\n",
    "for i in range(TEST_EXAMPLES):\n",
    "    X_test[i, 0:] = get_test_image_vector(i + 1)\n",
    "    y_test[i] = get_test_image_label(i + 1)\n",
    "\n",
    "y_vectors = np.zeros((len(y_test), 10))\n",
    "for i in range(len(y_test)):\n",
    "    y_vectors[i, int(y_test[i])] = 1\n",
    "\n",
    "X_test = X_test.T\n",
    "Y_test = y_vectors.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fVIiZHaGW_sA"
   },
   "source": [
    "#### Training Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 602
    },
    "colab_type": "code",
    "id": "iSgGels5XB9m",
    "outputId": "b94c5db3-7067-4beb-e4fb-d20a92affcf0"
   },
   "outputs": [],
   "source": [
    "ann = ANN(input_size=np.size(X_train, axis=0), cost_function='cross_entropy')\n",
    "ann.add_layer('relu', 100)\n",
    "ann.add_layer('relu', 100)\n",
    "ann.add_layer('sigmoid', 10)\n",
    "\n",
    "ann.fit(X_train, Y_train, num_iterations=350, learning_rate=0.000026, \n",
    "        regularization_constant=1.46, mini_batch_size=128, beta1=0.9, beta2=0.999)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ItaMzXEkUXTz"
   },
   "source": [
    "#### Evaluating network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 55
    },
    "colab_type": "code",
    "id": "vqRuoEyPrpTZ",
    "outputId": "a60b0055-801d-48c9-8895-7b13dc93b680"
   },
   "outputs": [],
   "source": [
    "train_set_predictions = ann_loaded.predict(X_train)\n",
    "predictions, correct_predictions = 0, 0\n",
    "for i in range(np.size(train_set_predictions, axis=1)):\n",
    "    prediction, prediction_prob = -1, - 1\n",
    "    for j in range(np.size(train_set_predictions, axis=0)):\n",
    "        if train_set_predictions[j, i] > prediction_prob:\n",
    "            prediction_prob = train_set_predictions[j, i]\n",
    "            prediction = j\n",
    "    if y_train[i] == prediction:\n",
    "        correct_predictions += 1\n",
    "    predictions += 1\n",
    "\n",
    "print('Network Training Accuracy: {}%'.format((correct_predictions * 100) / predictions))\n",
    "\n",
    "test_set_predictions = ann_loaded.predict(X_test)\n",
    "predictions, correct_predictions = 0, 0\n",
    "for i in range(np.size(test_set_predictions, axis=1)):\n",
    "    prediction, prediction_prob = -1, - 1\n",
    "    for j in range(np.size(test_set_predictions, axis=0)):\n",
    "        if test_set_predictions[j, i] > prediction_prob:\n",
    "            prediction_prob = test_set_predictions[j, i]\n",
    "            prediction = j\n",
    "    if y_test[i] == prediction:\n",
    "        correct_predictions += 1\n",
    "    predictions += 1\n",
    "\n",
    "print('Network Test Accuracy: {}%'.format((correct_predictions * 100) / predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UZPtJ2vfq98l"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "ANN_Improved.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
